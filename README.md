![University at Buffalo](http://www.nsm.buffalo.edu/Research/mcyulab//img/2-line_blue_gray.png)

***
# Training a linear regression model on LeToR and synthetic dataset using closed form solution and stochastic gradient descent

## Project Summary
***
We have trained linear regression model using closed form solution and stochastic
gradient descent.

* Firstly, we divided the given input and output data in three sets: Training (80%), Validation (10%) and Test (10%) using the random shuffle method.
* Then, we used grid search on M (number of clusters = number of basis functions) and ƛ (regularization coefficient).
* Using k-means clustering, derived M basis functions.
* Then, by three different learning techniques i.e. closed form solution, stochastic gradient descent without early stopping and SGD with early stopping, computed the weights for different values of M and ƛ on the training set.
* The model was then used to compute the validation set and root mean square error was computed.
* We tested the model on the testing set using M and ƛ for which we got least RMS error on validation set. The test error was computed which shows the amount of generalization achieved by learning.
* Above steps were done for both LeToR and synthetic datasets * We created a tabulation data for the varying values of lambda and clusters K, then we chose the lambda and clusters number where we got the minimum value in the validation error. Lowest validation error was returned by our code for each cluster by using simple if-else comparison. This was our tuning of parameters for the calculation of the test errors.
* Closed form solution throws memory error when we increase the value of M. The best method is to use SGD with early stopping.
* Since the closed form performs matrix multiplication, this will have higher time complexity compared to SGD.

## Approach
***
Using numpy’s genfromtxt function, the input and output data were read and stored into ‘X’ and ‘Y’ numpy array
* **Task - 1** : Split the input and output data into training (80%), validation (10%) and test set (10%) using train_test_split function of sklearn.model_selection. Let it be X_Train, X_Val, X_Test for inputs and Y_Train, Y_Val, Y_Test for outputs
* **Task - 2** : Use grid search on hyperparameters M and ƛ . Here, we have selected M, ranging from 10 to 75 with step size 5 in case of LeToR data and 3 to 13 with step size 1 in case of synthetic data. ƛ ranges from 0.1 to 2 with step size 0.1
* **Task - 3** : Divide X_Train in M number of clusters and computed its centroids using kmeans function of scipy library. Also, computed covariance matrix of each cluster which will be the spreads in each basis function.
* **Task - 4** : Compute Design Matrix for training, validation and test sets using the centroids and spreads obtained from Task - 3. This design matrix is a vector of M Gaussian radial basis functions. M is determined on the cluster numbers using Kmeans
clustering.
* **Task - 5** : For different ƛ values, compute M weights using either of the below three methods:
  1. Closed Form Solution: It takes ƛ , design matrix and output data as parameters and implements the closed form solution formula.
  2. Stochastic Gradient Descent without early stopping: Considered learning_rate = 1, minibatch_size=N (number of samples in training set), number of epochs=10000.
  3. Stochastic Gradient Descent with early stopping: In early stopping, for each epoch, we are calculating the validation RMS error. If the validation error in current epoch is lesser than the previous value, we consider the current epoch’s weight.
Also, suppose current validation error is greater than the previous one, we increment the counter j, which will be incremented on every incoming epoch cycles until a patience value (set to 10). Once it is equal to patience value, the SGD will be stopped and function will return the weights generated during the lowest validation error found till then.
* **Task - 6** : For each ƛ , we computed the RMS error for validation set (ErmsVal) and
considered the weights, ƛ and M for the minimum ErmsVal. Thus, our model has been
trained.
* **Task - 7**: Trained Model is then ran on the Test set with the optimum values of
hyperparameter obtained in Task - 6 and calculate the test error. It shows the
generalization efficiency of our model.

## Improvements
***
For efficient parameter tuning, below improvements were done:
1. Use of Early Stop to stop SGD process in case the validation error was not
decreasing anymore.
2. Averaging the outputs of K-Means clustering:While performing K-Means on the
training data set, we consider the average of the centroids generated by running
kmeans 10 times
3. Randomly shuffling of input and output data:
  a. Used train_test_split on input and output dataset, which divides the data set into training, validation and test set randomly. This helps in efficient training of model.

## Documentation
***
Report and documentation can be found on this [Documentation](https://github.com/jayantsolanki/574ML-project2-letor-linear-regression/blob/master/report/proj2.pdf) link

## Instructor
***
  * **Prof. Sargur N. Srihari**
  
## Teaching Assistants
***
  * **Jun Chu**
  * **Tianhang Zheng**
  * **Mengdi Huai**

## References
***
  * [Stackoverflow.com](Stackoverflow.com)
  * Python, Numpy and TensorFlow documentations
  
## License
***
This project is open-sourced under [MIT License](http://opensource.org/licenses/MIT)
